{"cells":[{"cell_type":"markdown","metadata":{"id":"-nB30hOvRppc"},"source":["# BillSum Summarization - Longform Encoder-Decoder (LED)"]},{"cell_type":"markdown","metadata":{"id":"4i2Z0LyERxbI"},"source":["## Load Dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78351,"status":"ok","timestamp":1754011403607,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"8Tj-77_DONSD","outputId":"16ccbc97-f77e-43ea-baec-30547b425ad1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: fsspec 2023.9.2\n","Uninstalling fsspec-2023.9.2:\n","  Successfully uninstalled fsspec-2023.9.2\n","\u001b[33mWARNING: Skipping gcsfs as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: transformers 4.54.1\n","Uninstalling transformers-4.54.1:\n","  Successfully uninstalled transformers-4.54.1\n","Found existing installation: torch 2.7.1\n","Uninstalling torch-2.7.1:\n","  Successfully uninstalled torch-2.7.1\n","Found existing installation: torchvision 0.22.1\n","Uninstalling torchvision-0.22.1:\n","  Successfully uninstalled torchvision-0.22.1\n","Found existing installation: torchaudio 2.7.1\n","Uninstalling torchaudio-2.7.1:\n","  Successfully uninstalled torchaudio-2.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 2.12.0 requires gcsfs!=2025.5.0,>=2023.3.0, which is not installed.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.7.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.9.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.1)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n","Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.7.1)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.54.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2023.9.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.11.1.6)\n","Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.3.1)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=1.0.0->bert_score) (75.2.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.34.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.7.14)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert_score) (1.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n"]}],"source":["!pip uninstall -y flash-attn fsspec gcsfs transformers torch torchvision torchaudio\n","!pip install -q -U transformers accelerate bitsandbytes torch torchvision torchaudio datasets==2.14.6 fsspec==2023.9.2\n","!pip install -q -U rouge_score evaluate\n","!pip install -U sentence-transformers\n","!pip install bert_score"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1754014614518,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"eLy9x2UUM9tS"},"outputs":[],"source":["import datasets\n","from transformers import LEDTokenizer, LEDForConditionalGeneration, AutoConfig, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","import evaluate\n","from tqdm import tqdm\n","import torch, gc\n","from itertools import islice\n","from torch.utils.data import IterableDataset\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sentence_transformers import SentenceTransformer\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","import numpy as np\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"markdown","metadata":{"id":"s6mYRzkgR2Es"},"source":["## Load BillSum Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5636,"status":"ok","timestamp":1754011517824,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"VR1DuY7VOFKd","outputId":"b2f330aa-dba9-473e-acb4-ef9154e5bc9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3269, 3)\n"]}],"source":["billsum_train = datasets.load_dataset(\"billsum\", split = \"train\", streaming = True)\n","billsum_test = datasets.load_dataset(\"billsum\", split = \"test\")\n","\n","print(billsum_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"PQZN9IsRS-B-"},"source":["# Text Separation and Splitting"]},{"cell_type":"markdown","metadata":{"id":"1cAoES-uTeuS"},"source":["The following functions split a bill into its main numbered sections and then further splits them based off of each lettered subsection. We are not splitting by each sentence because the text formmatting makes it difficult to split it that way."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1754011528646,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"sJkFrzzro2Z1"},"outputs":[],"source":["def split_bill_into_sections(text):\n","    # Normalize line breaks\n","    text = re.sub(r'\\n+', '\\n', text)\n","\n","    # Ensure section headers start on a new line (case-insensitive)\n","    text = re.sub(r'(?<!\\n)((SECTION|SEC)\\.?\\s*\\d+\\.)', r'\\n\\1', text, flags=re.IGNORECASE)\n","\n","    # Match from each SECTION/SEC until the next one or end of text\n","    pattern = r'((?:SEC|SECTION)\\.?\\s*\\d+\\..*?)(?=(?:SEC|SECTION)\\.?\\s*\\d+\\.|\\Z)'\n","    sections = re.findall(pattern, text, flags=re.DOTALL | re.IGNORECASE)\n","\n","    if len(sections) == 0:\n","        return [text]\n","\n","    return sections\n","\n","\n","def split_by_lettered_clause_with_linebreaks(text):\n","    # Insert a unique delimiter before (a), (b), ... only if they occur after a line break and spaces\n","    pattern = r'\\n[ \\t]*\\([a-z]\\)(?!\\([a-zA-Z0-9]\\))'\n","    split_text = re.split(f'({pattern})', text)\n","\n","    # Recombine so that each clause includes its clause marker\n","    combined = []\n","    buffer = ''\n","    for part in split_text:\n","        if re.match(pattern, part):\n","            if buffer:\n","                combined.append(buffer.strip())\n","            buffer = part.lstrip('\\n')\n","        else:\n","            buffer += part\n","    if buffer:\n","        combined.append(buffer.strip())\n","\n","    res = [clause for clause in combined if clause]\n","    if res == []:\n","        return [text]\n","    return res\n","\n","\n","# helper function for splitting the bill into subsections\n","def split_bill(text):\n","    sections = split_bill_into_sections(text)\n","    nested_list = [split_by_lettered_clause_with_linebreaks(section) for section in sections]\n","    flat_list = [item for sublist in nested_list for item in sublist]\n","    return list(dict.fromkeys(flat_list)) # prevent duplicates"]},{"cell_type":"markdown","metadata":{"id":"Vw7VaH1QTu1Z"},"source":["# Cluster Based Extraction\n","\n","Below is a function for cluster based extraction. For the vectorization of our sentences we are using tfidf."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1754011534896,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"0QH4DF37jivN"},"outputs":[],"source":["def cluster_based_summary(sentences, num_clusters = 3):\n","    vectorizer = TfidfVectorizer(stop_words = 'english')\n","    X = vectorizer.fit_transform(sentences)\n","\n","    # Adjust num_clusters if it's greater than the number of sentences or if X is empty\n","    if len(sentences) == 0:\n","        return []\n","    num_clusters = min(num_clusters, len(sentences))\n","\n","    # Handle the case where X might have only one sample (resulting in a single cluster)\n","    if num_clusters == 1:\n","         return [sentences[0]]\n","\n","    kmeans = KMeans(n_clusters = num_clusters, random_state = 0, n_init=10) # Add n_init for robustness\n","    kmeans.fit(X)\n","\n","    # Find closest sentence to each cluster centroid\n","    summary_sentences = []\n","    for i in range(num_clusters):\n","        cluster_center = kmeans.cluster_centers_[i]\n","        cluster_indices = np.where(kmeans.labels_ == i)[0]\n","\n","        # Check if the cluster is not empty\n","        if len(cluster_indices) > 0:\n","            cluster_vectors = X[cluster_indices].toarray()\n","\n","            # Compute distances to centroid\n","            distances = np.linalg.norm(cluster_vectors - cluster_center, axis = 1)\n","            closest_index = cluster_indices[np.argmin(distances)]\n","            summary_sentences.append(sentences[closest_index])\n","\n","    # Sort by original order\n","    # Only attempt to sort if summary_sentences is not empty\n","    if summary_sentences:\n","        summary_sentences.sort(key = lambda s: sentences.index(s))\n","\n","    return summary_sentences"]},{"cell_type":"markdown","metadata":{"id":"bx5cz0EBba5P"},"source":["# Example on the first bill\n","\n","Below we perform cluster based extraction on the first bill in the test dataset. We split the bill into subsections as mentioned above and defined eight cluters. The result is an eight sentence summary of the bill."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1754011537330,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"VkDFOfdKq6Xe","outputId":"edd9ef3c-e051-4e68-c9e7-f82c3595ec6a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['SECTION 1. ENVIRONMENTAL INFRASTRUCTURE.',\n"," \"(a) Jackson County, Mississippi.--Section 219 of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended--\\n        (1) in subsection (c), by striking paragraph (5) and inserting \\n    the following:\\n        ``(5) Jackson county, mississippi.--Provision of an alternative \\n    water supply and a project for the elimination or control of \\n    combined sewer overflows for Jackson County, Mississippi.''; and\\n        (2) in subsection (e)(1), by striking ``$10,000,000'' and \\n    inserting ``$20,000,000''.\",\n"," \"(b) Manchester, New Hampshire.--Section 219(e)(3) of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended by striking ``$10,000,000'' and inserting ``$20,000,000''.\",\n"," \"(c) Atlanta, Georgia.--Section 219(f)(1) of the Water Resources \\nDevelopment Act of 1992 (106 Stat. 4835; 113 Stat. 335) is amended by \\nstriking ``$25,000,000 for''.\",\n"," \"(d) Paterson, Passaic County, and Passaic Valley, New Jersey.--\\nSection 219(f)(2) of the Water Resources Development Act of 1992 (106 \\nStat. 4835; 113 Stat. 335) is amended by striking ``$20,000,000 for''.\",\n"," \"SEC. 2. UPPER MISSISSIPPI RIVER ENVIRONMENTAL MANAGEMENT PROGRAM.\\n    Section 1103(e)(5) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 652(e)(5)) (as amended by section 509(c)(3) of the Water \\nResources Development Act of 1999 (113 Stat. 340)) is amended by \\nstriking ``paragraph (1)(A)(i)'' and inserting ``paragraph (1)(B)''.\",\n"," \"SEC. 3. DELAWARE RIVER, PENNSYLVANIA AND DELAWARE.\\n    Section 346 of the Water Resources Development Act of 1999 (113 \\nStat. 309) is amended by striking ``economically acceptable'' and \\ninserting ``environmentally acceptable''.\",\n"," \"SEC. 4. PROJECT REAUTHORIZATIONS.\\n    Section 364 of the Water Resources Development Act of 1999 (113 \\nStat. 313) is amended--\\n        (1) by striking ``Each'' and all that follows through the colon \\n    and inserting the following: ``Each of the following projects is \\n    authorized to be carried out by the Secretary, and no construction \\n    on any such project may be initiated until the Secretary determines \\n    that the project is technically sound, environmentally acceptable, \\n    and economically justified:'';\\n        (2) by striking paragraph (1); and\\n        (3) by redesignating paragraphs (2) through (6) as paragraphs \\n    (1) through (5), respectively.\",\n"," \"SEC. 5. SHORE PROTECTION.\\n    Section 103(d)(2)(A) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 2213(d)(2)(A)) (as amended by section 215(a)(2) of the Water \\nResources Development Act of 1999 (113 Stat. 292)) is amended by \\nstriking ``or for which a feasibility study is completed after that \\ndate,'' and inserting ``except for a project for which a District \\nEngineer's Report is completed by that date,''.\",\n"," \"SEC. 6. COMITE RIVER, LOUISIANA.\\n    Section 371 of the Water Resources Development Act of 1999 (113 \\nStat. 321) is amended--\\n        (1) by inserting ``(a) In General.--'' before ``The''; and\\n        (2) by adding at the end the following:\\n    ``(b) Crediting of Reduction in Non-Federal Share.--The project \\ncooperation agreement for the Comite River Diversion Project shall \\ninclude a provision that specifies that any reduction in the non-\\nFederal share that results from the modification under subsection (a) \\nshall be credited toward the share of project costs to be paid by the \\nAmite River Basin Drainage and Water Conservation District.''.\",\n"," \"SEC. 7. CHESAPEAKE CITY, MARYLAND.\\n    Section 535(b) of the Water Resources Development Act of 1999 (113 \\nStat. 349) is amended by striking ``the city of Chesapeake'' each place \\nit appears and inserting ``Chesapeake City''.\",\n"," 'SEC. 8. CONTINUATION OF SUBMISSION OF CERTAIN REPORTS BY THE SECRETARY \\n              OF THE ARMY.',\n"," \"(c) Reports on Participation of Minority Groups and Minority-Owned \\nFirms in Mississippi River-Gulf Outlet Feature.--Section 844(b) of the \\nWater Resources Development Act of 1986 (100 Stat. 4177) is amended in \\nthe second sentence by striking ``The'' and inserting ``Notwithstanding \\nsection 3003 of Public Law 104-66 (31 U.S.C. 1113 note; 109 Stat. 734), \\nthe''.\",\n"," \"(d) List of Authorized but Unfunded Projects.--Section 1001(b)(2) \\nof the Water Resources Development Act of 1986 (33 U.S.C. 579a(b)(2)) \\nis amended in the first sentence by striking ``Every'' and inserting \\n``Notwithstanding section 3003 of Public Law 104-66 (31 U.S.C. 1113 \\nnote; 109 Stat. 734), every''.\",\n"," 'SEC. 9. AUTHORIZATIONS FOR PROGRAM PREVIOUSLY AND CURRENTLY FUNDED.',\n"," '(a) Program Authorization.--The program described in subsection (c) \\nis hereby authorized.',\n"," '(b) Authorization of Appropriations.--Funds are hereby authorized \\nto be appropriated for the Department of Transportation for the program \\nauthorized in subsection (a) in amounts as follows:\\n        (1) Fiscal year 2000.--For fiscal year 2000, $10,000,000.\\n        (2) Fiscal year 2001.--For fiscal year 2001, $10,000,000.\\n        (3) Fiscal year 2002.--For fiscal year 2002, $7,000,000.',\n"," \"(c) Applicability.--The program referred to in subsection (a) is \\nthe program for which funds appropriated in title I of Public Law 106-\\n69 under the heading ``FEDERAL RAILROAD ADMINISTRATION'' are available \\nfor obligation upon the enactment of legislation authorizing the \\nprogram.\\n                               Speaker of the House of Representatives.\\n                            Vice President of the United States and    \\n                                               President of the Senate.\"]"]},"metadata":{},"execution_count":10}],"source":["# Example\n","test_split = split_bill(billsum_test[0][\"text\"])\n","cluster_based_summary(test_split, 18)"]},{"cell_type":"markdown","metadata":{"id":"EeLqggBCcDvg"},"source":["The below function performs cluster based extraction on a list of paragraphs with a set number of clusters, returning a list of summaries."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1754011539193,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"US3LDD3KMcaB"},"outputs":[],"source":["def do_cluster_based_summary(paragraphs, num_clusters = 3):\n","    res = []\n","    for i in range(len(paragraphs)):\n","        paragraph = paragraphs[i]\n","        split = split_bill(paragraph)\n","\n","        if len(split) < num_clusters:\n","            summary = cluster_based_summary(split, len(split))\n","        elif len(split) == num_clusters:\n","            summary = split\n","        else:\n","            summary = cluster_based_summary(split, num_clusters)\n","\n","        processed_summary = \" \".join(summary)\n","        res.append(processed_summary)\n","\n","    return res"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":627},"executionInfo":{"elapsed":283586,"status":"ok","timestamp":1754011824000,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"mf2IjFExNyOg","outputId":"b050ff6d-0446-40d7-f1bd-71f6422dac16"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (17). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n"]},{"output_type":"execute_result","data":{"text/plain":["\"SECTION 1. ENVIRONMENTAL INFRASTRUCTURE. (a) Jackson County, Mississippi.--Section 219 of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended--\\n        (1) in subsection (c), by striking paragraph (5) and inserting \\n    the following:\\n        ``(5) Jackson county, mississippi.--Provision of an alternative \\n    water supply and a project for the elimination or control of \\n    combined sewer overflows for Jackson County, Mississippi.''; and\\n        (2) in subsection (e)(1), by striking ``$10,000,000'' and \\n    inserting ``$20,000,000''. (b) Manchester, New Hampshire.--Section 219(e)(3) of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended by striking ``$10,000,000'' and inserting ``$20,000,000''. (c) Atlanta, Georgia.--Section 219(f)(1) of the Water Resources \\nDevelopment Act of 1992 (106 Stat. 4835; 113 Stat. 335) is amended by \\nstriking ``$25,000,000 for''. (d) Paterson, Passaic County, and Passaic Valley, New Jersey.--\\nSection 219(f)(2) of the Water Resources Development Act of 1992 (106 \\nStat. 4835; 113 Stat. 335) is amended by striking ``$20,000,000 for''. SEC. 2. UPPER MISSISSIPPI RIVER ENVIRONMENTAL MANAGEMENT PROGRAM.\\n    Section 1103(e)(5) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 652(e)(5)) (as amended by section 509(c)(3) of the Water \\nResources Development Act of 1999 (113 Stat. 340)) is amended by \\nstriking ``paragraph (1)(A)(i)'' and inserting ``paragraph (1)(B)''. SEC. 3. DELAWARE RIVER, PENNSYLVANIA AND DELAWARE.\\n    Section 346 of the Water Resources Development Act of 1999 (113 \\nStat. 309) is amended by striking ``economically acceptable'' and \\ninserting ``environmentally acceptable''. SEC. 4. PROJECT REAUTHORIZATIONS.\\n    Section 364 of the Water Resources Development Act of 1999 (113 \\nStat. 313) is amended--\\n        (1) by striking ``Each'' and all that follows through the colon \\n    and inserting the following: ``Each of the following projects is \\n    authorized to be carried out by the Secretary, and no construction \\n    on any such project may be initiated until the Secretary determines \\n    that the project is technically sound, environmentally acceptable, \\n    and economically justified:'';\\n        (2) by striking paragraph (1); and\\n        (3) by redesignating paragraphs (2) through (6) as paragraphs \\n    (1) through (5), respectively. SEC. 5. SHORE PROTECTION.\\n    Section 103(d)(2)(A) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 2213(d)(2)(A)) (as amended by section 215(a)(2) of the Water \\nResources Development Act of 1999 (113 Stat. 292)) is amended by \\nstriking ``or for which a feasibility study is completed after that \\ndate,'' and inserting ``except for a project for which a District \\nEngineer's Report is completed by that date,''. SEC. 6. COMITE RIVER, LOUISIANA.\\n    Section 371 of the Water Resources Development Act of 1999 (113 \\nStat. 321) is amended--\\n        (1) by inserting ``(a) In General.--'' before ``The''; and\\n        (2) by adding at the end the following:\\n    ``(b) Crediting of Reduction in Non-Federal Share.--The project \\ncooperation agreement for the Comite River Diversion Project shall \\ninclude a provision that specifies that any reduction in the non-\\nFederal share that results from the modification under subsection (a) \\nshall be credited toward the share of project costs to be paid by the \\nAmite River Basin Drainage and Water Conservation District.''. SEC. 7. CHESAPEAKE CITY, MARYLAND.\\n    Section 535(b) of the Water Resources Development Act of 1999 (113 \\nStat. 349) is amended by striking ``the city of Chesapeake'' each place \\nit appears and inserting ``Chesapeake City''. SEC. 8. CONTINUATION OF SUBMISSION OF CERTAIN REPORTS BY THE SECRETARY \\n              OF THE ARMY. (c) Reports on Participation of Minority Groups and Minority-Owned \\nFirms in Mississippi River-Gulf Outlet Feature.--Section 844(b) of the \\nWater Resources Development Act of 1986 (100 Stat. 4177) is amended in \\nthe second sentence by striking ``The'' and inserting ``Notwithstanding \\nsection 3003 of Public Law 104-66 (31 U.S.C. 1113 note; 109 Stat. 734), \\nthe''. (d) List of Authorized but Unfunded Projects.--Section 1001(b)(2) \\nof the Water Resources Development Act of 1986 (33 U.S.C. 579a(b)(2)) \\nis amended in the first sentence by striking ``Every'' and inserting \\n``Notwithstanding section 3003 of Public Law 104-66 (31 U.S.C. 1113 \\nnote; 109 Stat. 734), every''. SEC. 9. AUTHORIZATIONS FOR PROGRAM PREVIOUSLY AND CURRENTLY FUNDED. (a) Program Authorization.--The program described in subsection (c) \\nis hereby authorized. (b) Authorization of Appropriations.--Funds are hereby authorized \\nto be appropriated for the Department of Transportation for the program \\nauthorized in subsection (a) in amounts as follows:\\n        (1) Fiscal year 2000.--For fiscal year 2000, $10,000,000.\\n        (2) Fiscal year 2001.--For fiscal year 2001, $10,000,000.\\n        (3) Fiscal year 2002.--For fiscal year 2002, $7,000,000. (c) Applicability.--The program referred to in subsection (a) is \\nthe program for which funds appropriated in title I of Public Law 106-\\n69 under the heading ``FEDERAL RAILROAD ADMINISTRATION'' are available \\nfor obligation upon the enactment of legislation authorizing the \\nprogram.\\n                               Speaker of the House of Representatives.\\n                            Vice President of the United States and    \\n                                               President of the Senate.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["billsum_test_list = [bill[\"text\"] for bill in billsum_test]\n","\n","test = do_cluster_based_summary(billsum_test_list, 18)\n","test[0]"]},{"cell_type":"markdown","metadata":{"id":"pPwUsqKIMJIO"},"source":["This helper compresses single bills strings into k clauses."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1754011827731,"user":{"displayName":"Chris John","userId":"17204747059348754073"},"user_tz":300},"id":"KYVNPy7oMEPH"},"outputs":[],"source":["def compress_bill(text: str, k: int = 8) -> str:\n","    clauses = split_bill(text)\n","    if len(clauses) == 0:\n","        return text[:1000]\n","\n","    chosen = clauses if len(clauses) <= k else cluster_based_summary(clauses, k)\n","    return \" \".join(chosen)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"id":"QuM556uNPRC3","executionInfo":{"status":"ok","timestamp":1754011929323,"user_tz":300,"elapsed":100448,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"2c656e7a-3d28-46a5-e6bd-77aededb8daa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"SECTION 1. ENVIRONMENTAL INFRASTRUCTURE. (a) Jackson County, Mississippi.--Section 219 of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended--\\n        (1) in subsection (c), by striking paragraph (5) and inserting \\n    the following:\\n        ``(5) Jackson county, mississippi.--Provision of an alternative \\n    water supply and a project for the elimination or control of \\n    combined sewer overflows for Jackson County, Mississippi.''; and\\n        (2) in subsection (e)(1), by striking ``$10,000,000'' and \\n    inserting ``$20,000,000''. (b) Manchester, New Hampshire.--Section 219(e)(3) of the Water \\nResources Development Act of 1992 (106 Stat. 4835; 110 Stat. 3757) is \\namended by striking ``$10,000,000'' and inserting ``$20,000,000''. (c) Atlanta, Georgia.--Section 219(f)(1) of the Water Resources \\nDevelopment Act of 1992 (106 Stat. 4835; 113 Stat. 335) is amended by \\nstriking ``$25,000,000 for''. (d) Paterson, Passaic County, and Passaic Valley, New Jersey.--\\nSection 219(f)(2) of the Water Resources Development Act of 1992 (106 \\nStat. 4835; 113 Stat. 335) is amended by striking ``$20,000,000 for''. SEC. 2. UPPER MISSISSIPPI RIVER ENVIRONMENTAL MANAGEMENT PROGRAM.\\n    Section 1103(e)(5) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 652(e)(5)) (as amended by section 509(c)(3) of the Water \\nResources Development Act of 1999 (113 Stat. 340)) is amended by \\nstriking ``paragraph (1)(A)(i)'' and inserting ``paragraph (1)(B)''. SEC. 3. DELAWARE RIVER, PENNSYLVANIA AND DELAWARE.\\n    Section 346 of the Water Resources Development Act of 1999 (113 \\nStat. 309) is amended by striking ``economically acceptable'' and \\ninserting ``environmentally acceptable''. SEC. 4. PROJECT REAUTHORIZATIONS.\\n    Section 364 of the Water Resources Development Act of 1999 (113 \\nStat. 313) is amended--\\n        (1) by striking ``Each'' and all that follows through the colon \\n    and inserting the following: ``Each of the following projects is \\n    authorized to be carried out by the Secretary, and no construction \\n    on any such project may be initiated until the Secretary determines \\n    that the project is technically sound, environmentally acceptable, \\n    and economically justified:'';\\n        (2) by striking paragraph (1); and\\n        (3) by redesignating paragraphs (2) through (6) as paragraphs \\n    (1) through (5), respectively. SEC. 5. SHORE PROTECTION.\\n    Section 103(d)(2)(A) of the Water Resources Development Act of 1986 \\n(33 U.S.C. 2213(d)(2)(A)) (as amended by section 215(a)(2) of the Water \\nResources Development Act of 1999 (113 Stat. 292)) is amended by \\nstriking ``or for which a feasibility study is completed after that \\ndate,'' and inserting ``except for a project for which a District \\nEngineer's Report is completed by that date,''. SEC. 6. COMITE RIVER, LOUISIANA.\\n    Section 371 of the Water Resources Development Act of 1999 (113 \\nStat. 321) is amended--\\n        (1) by inserting ``(a) In General.--'' before ``The''; and\\n        (2) by adding at the end the following:\\n    ``(b) Crediting of Reduction in Non-Federal Share.--The project \\ncooperation agreement for the Comite River Diversion Project shall \\ninclude a provision that specifies that any reduction in the non-\\nFederal share that results from the modification under subsection (a) \\nshall be credited toward the share of project costs to be paid by the \\nAmite River Basin Drainage and Water Conservation District.''. SEC. 7. CHESAPEAKE CITY, MARYLAND.\\n    Section 535(b) of the Water Resources Development Act of 1999 (113 \\nStat. 349) is amended by striking ``the city of Chesapeake'' each place \\nit appears and inserting ``Chesapeake City''. SEC. 8. CONTINUATION OF SUBMISSION OF CERTAIN REPORTS BY THE SECRETARY \\n              OF THE ARMY. (c) Reports on Participation of Minority Groups and Minority-Owned \\nFirms in Mississippi River-Gulf Outlet Feature.--Section 844(b) of the \\nWater Resources Development Act of 1986 (100 Stat. 4177) is amended in \\nthe second sentence by striking ``The'' and inserting ``Notwithstanding \\nsection 3003 of Public Law 104-66 (31 U.S.C. 1113 note; 109 Stat. 734), \\nthe''. (d) List of Authorized but Unfunded Projects.--Section 1001(b)(2) \\nof the Water Resources Development Act of 1986 (33 U.S.C. 579a(b)(2)) \\nis amended in the first sentence by striking ``Every'' and inserting \\n``Notwithstanding section 3003 of Public Law 104-66 (31 U.S.C. 1113 \\nnote; 109 Stat. 734), every''. SEC. 9. AUTHORIZATIONS FOR PROGRAM PREVIOUSLY AND CURRENTLY FUNDED. (a) Program Authorization.--The program described in subsection (c) \\nis hereby authorized. (b) Authorization of Appropriations.--Funds are hereby authorized \\nto be appropriated for the Department of Transportation for the program \\nauthorized in subsection (a) in amounts as follows:\\n        (1) Fiscal year 2000.--For fiscal year 2000, $10,000,000.\\n        (2) Fiscal year 2001.--For fiscal year 2001, $10,000,000.\\n        (3) Fiscal year 2002.--For fiscal year 2002, $7,000,000. (c) Applicability.--The program referred to in subsection (a) is \\nthe program for which funds appropriated in title I of Public Law 106-\\n69 under the heading ``FEDERAL RAILROAD ADMINISTRATION'' are available \\nfor obligation upon the enactment of legislation authorizing the \\nprogram.\\n                               Speaker of the House of Representatives.\\n                            Vice President of the United States and    \\n                                               President of the Senate.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["compressed_test_list = [compress_bill(bill[\"text\"], k = 18) for bill in billsum_test]\n","compressed_test_list[0]"]},{"cell_type":"markdown","metadata":{"id":"v3NklNZR_v1O"},"source":["# Evaluation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9lOWQWMZFWZw"},"source":["Below is code to calculate the ROUGE score."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raz9obkSAGXb","executionInfo":{"status":"ok","timestamp":1754012535235,"user_tz":300,"elapsed":454204,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"8fc674d1-1ffe-46c2-c44d-07bf340afd1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'rouge1': np.float64(0.23214588014002258), 'rouge2': np.float64(0.1581086665125567), 'rougeL': np.float64(0.18452364692758716), 'rougeLsum': np.float64(0.21454869438068427)}\n"]}],"source":["rouge = evaluate.load('rouge')\n","\n","predictions = test\n","references = [bill[\"summary\"] for bill in billsum_test]\n","\n","results = rouge.compute(\n","    predictions = predictions,\n","    references = references)\n","\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"hE6YRt5kFtD4"},"source":["Below is code to calculate the BERTScore. We calculate on the first 100 bills because collab runs out of RAM and crashes."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"RNOs1YT3Frm7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754012559407,"user_tz":300,"elapsed":24171,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"cf728224-ca14-430b-a859-d23c3c1c7deb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n","  return forward_call(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["{'precision': [0.7528371810913086, 0.7410010099411011, 0.750971794128418, 0.6871652603149414, 0.768215000629425, 0.7201582789421082, 0.7323994636535645, 0.733079731464386, 0.7162339687347412, 0.7147027850151062, 0.6808777451515198, 0.7714466452598572, 0.6924551725387573, 0.7429553270339966, 0.72789466381073, 0.7764732241630554, 0.718122661113739, 0.7408742904663086, 0.808912992477417, 0.716727077960968, 0.7985923290252686, 0.7448854446411133, 0.7304651737213135, 0.7855857610702515, 0.7067907452583313, 0.7153390645980835, 0.752629280090332, 0.7258044481277466, 0.7238776683807373, 0.7596798539161682, 0.7059012055397034, 0.728073000907898, 0.7322126626968384, 0.7197351455688477, 0.7607904672622681, 0.7035062313079834, 0.7059399485588074, 0.7723203897476196, 0.7945020794868469, 0.6999794840812683, 0.7143810987472534, 0.7924513220787048, 0.7372660636901855, 0.6775892376899719, 0.7576332092285156, 0.6937090158462524, 0.7232735753059387, 0.7751209735870361, 0.7911643981933594, 0.7487556338310242, 0.6673814058303833, 0.743767261505127, 0.745256781578064, 0.7638019323348999, 0.7681575417518616, 0.7724125981330872, 0.6960351467132568, 0.6714919805526733, 0.7388262748718262, 0.6612454652786255, 0.7812170386314392, 0.699063241481781, 0.7062414288520813, 0.7544940114021301, 0.73125821352005, 0.6732702255249023, 0.7375917434692383, 0.6947348117828369, 0.7168666124343872, 0.7600144743919373, 0.7060958743095398, 0.7532115578651428, 0.7089033126831055, 0.7345552444458008, 0.7360330820083618, 0.7830080389976501, 0.808684229850769, 0.7571908235549927, 0.7130424380302429, 0.7670373916625977, 0.7629841566085815, 0.7465029954910278, 0.7720062136650085, 0.6953787803649902, 0.8067518472671509, 0.7359049320220947, 0.7339203357696533, 0.8345375061035156, 0.7473341226577759, 0.7650045156478882, 0.7084475159645081, 0.8413447141647339, 0.7337079048156738, 0.7762675881385803, 0.6780646443367004, 0.7476109862327576, 0.690082311630249, 0.6808333396911621, 0.766002357006073, 0.7067188024520874], 'recall': [0.8003660440444946, 0.8603482842445374, 0.8502481579780579, 0.8308120965957642, 0.8684036135673523, 0.8541393280029297, 0.8496441841125488, 0.8292683362960815, 0.8377040028572083, 0.8404278755187988, 0.8283870220184326, 0.8252686262130737, 0.8487263917922974, 0.8686147928237915, 0.8271195888519287, 0.8291088342666626, 0.869674563407898, 0.8415431976318359, 0.8763189911842346, 0.8715310096740723, 0.8799862861633301, 0.7904903888702393, 0.8441592454910278, 0.8541322946548462, 0.845569908618927, 0.840422511100769, 0.8570665717124939, 0.8468422889709473, 0.8450311422348022, 0.8974875211715698, 0.8230341672897339, 0.8615026473999023, 0.8848203420639038, 0.8275331258773804, 0.8266608715057373, 0.813672661781311, 0.8210369348526001, 0.896136462688446, 0.8524629473686218, 0.847915768623352, 0.8152509331703186, 0.8345284461975098, 0.8960632085800171, 0.793892502784729, 0.8048425912857056, 0.8307315111160278, 0.8569250702857971, 0.867046058177948, 0.8455181121826172, 0.8348819613456726, 0.8651086688041687, 0.8548590540885925, 0.8780032992362976, 0.8194650411605835, 0.8119089603424072, 0.8623559474945068, 0.8287454843521118, 0.8802822232246399, 0.8123739957809448, 0.8683302402496338, 0.8260305523872375, 0.8498874306678772, 0.8338528871536255, 0.8633843660354614, 0.8575862646102905, 0.8533024191856384, 0.8278387784957886, 0.8426439762115479, 0.8500319719314575, 0.8714578151702881, 0.8430787324905396, 0.8542718887329102, 0.8624520897865295, 0.8371148705482483, 0.8252835273742676, 0.8863512277603149, 0.8335256576538086, 0.834707498550415, 0.8702021241188049, 0.8369225859642029, 0.8140048384666443, 0.8310494422912598, 0.7985669374465942, 0.8385949730873108, 0.8581851720809937, 0.8487134575843811, 0.8143212795257568, 0.8212943077087402, 0.8473418951034546, 0.8085820078849792, 0.871587872505188, 0.8509705066680908, 0.8109036684036255, 0.8127185702323914, 0.8575221300125122, 0.8740056753158569, 0.8221104145050049, 0.8437291383743286, 0.9100168347358704, 0.8060131669044495], 'f1': [0.7758744359016418, 0.7962271571159363, 0.7975323796272278, 0.7521920204162598, 0.8152427077293396, 0.7814475297927856, 0.7866773009300232, 0.7782130241394043, 0.7722213864326477, 0.7724831700325012, 0.7474239468574524, 0.797450602054596, 0.7626680135726929, 0.8008860349655151, 0.774341344833374, 0.8019282817840576, 0.7866659760475159, 0.788006603717804, 0.8412679433822632, 0.7865849137306213, 0.8373159766197205, 0.7670106291770935, 0.7832076549530029, 0.818426251411438, 0.7699769735336304, 0.7728524208068848, 0.8014599084854126, 0.781665563583374, 0.7797765731811523, 0.822853684425354, 0.7599808573722839, 0.7891877889633179, 0.8013153672218323, 0.7698789834976196, 0.7923590540885925, 0.7545897364616394, 0.7591507434844971, 0.8296342492103577, 0.8224626183509827, 0.7668783068656921, 0.7614901065826416, 0.8129457831382751, 0.8089452981948853, 0.7311446070671082, 0.780524730682373, 0.7560622096061707, 0.7844473123550415, 0.8185105919837952, 0.8174387216567993, 0.789476752281189, 0.7534893155097961, 0.7954531311988831, 0.806202232837677, 0.7906550168991089, 0.7894275188446045, 0.8149099946022034, 0.7566150426864624, 0.7618408203125, 0.7738565802574158, 0.7507694959640503, 0.8029990792274475, 0.7671322822570801, 0.7647601962089539, 0.8052747249603271, 0.789400041103363, 0.7526705265045166, 0.7801138758659363, 0.7615743279457092, 0.7777906656265259, 0.8119298815727234, 0.768531084060669, 0.8005650043487549, 0.7781755328178406, 0.7824888229370117, 0.7781074047088623, 0.8314809203147888, 0.820917010307312, 0.7940618395805359, 0.7838220596313477, 0.8004574775695801, 0.7876691818237305, 0.7865106463432312, 0.7850620150566101, 0.7603013515472412, 0.8316740393638611, 0.7882937788963318, 0.772033154964447, 0.8278629183769226, 0.7942020893096924, 0.7861899137496948, 0.7815954685211182, 0.8461302518844604, 0.7703767418861389, 0.7940750122070312, 0.7573071122169495, 0.8058825135231018, 0.750332772731781, 0.7535787224769592, 0.8318222761154175, 0.7531071901321411], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.54.1)'}\n"]}],"source":["bertscore = evaluate.load(\"bertscore\")\n","\n","predictions = test[:100] # first 100 summaries\n","references = [bill[\"summary\"] for bill in billsum_test][:100]\n","\n","results = bertscore.compute(\n","    predictions = predictions,\n","    references = references,\n","    lang = \"en\")\n","\n","print(results)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"mHr4xni8Mfji","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754012769213,"user_tz":300,"elapsed":47,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"8ac7910f-53a1-4d4b-e84f-70fb74d60240"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average precision: 0.7375175493955612\n","Average recall: 0.8442489129304885\n","Average f1: 0.7866578328609467\n"]}],"source":["print(f'Average precision: {np.mean(results[\"precision\"])}')\n","print(f'Average recall: {np.mean(results[\"recall\"])}')\n","print(f'Average f1: {np.mean(results[\"f1\"])}')"]},{"cell_type":"markdown","metadata":{"id":"bASz6GyDNQUN"},"source":["# Hybrid Cluster Extraction + LED"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"um4GaVkn2Dsj","executionInfo":{"status":"ok","timestamp":1754012773113,"user_tz":300,"elapsed":14,"user":{"displayName":"Chris John","userId":"17204747059348754073"}}},"outputs":[],"source":["def get_cluster_summary_as_dataset(dataset, num_clusters = 3):\n","    sentences = [bill[\"text\"] for bill in dataset]\n","    summaries = [bill[\"summary\"] for bill in dataset]\n","\n","    cluster_summaries = do_cluster_based_summary(sentences, num_clusters)\n","\n","    res = {\"text\": cluster_summaries, \"summary\": summaries}\n","    return datasets.Dataset.from_dict(res)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"tsCcGFT44Zpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013048869,"user_tz":300,"elapsed":274047,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"19f3366c-a9b0-4779-997d-e18a77f7dd25"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (15). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (17) found smaller than n_clusters (18). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (13) found smaller than n_clusters (14). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (17). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (11) found smaller than n_clusters (12). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (14) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (13). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (15) found smaller than n_clusters (16). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (12) found smaller than n_clusters (14). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n"]},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['text', 'summary'],\n","    num_rows: 3269\n","})"]},"metadata":{},"execution_count":19}],"source":["test = get_cluster_summary_as_dataset(billsum_test, 18)\n","test"]},{"cell_type":"markdown","metadata":{"id":"5F6oqBneJLEo"},"source":["### Check Max Sequence Length\n","\n","LED has a maximum sequence length of 16,384, but due to memory constraints, we'll cap that number to 4,096.\n","\n","We can tokenize a subset of the training data to check how many documents will be truncated."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"7j-KnwFYIrGS","executionInfo":{"status":"ok","timestamp":1754013114969,"user_tz":300,"elapsed":595,"user":{"displayName":"Chris John","userId":"17204747059348754073"}}},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model_name = \"allenai/led-base-16384\"\n","\n","tokenizer = LEDTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"0o5x43Bp43Cq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013153435,"user_tz":300,"elapsed":34272,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"517bee49-2851-402d-fb72-77983efddb5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 1000/1000 [00:34<00:00, 29.18it/s]\n"]}],"source":["chunk_size = 1000\n","text_lengths = []\n","\n","for record in tqdm(islice(billsum_train, chunk_size), total = chunk_size):\n","  text_lengths.append(len(tokenizer(record[\"text\"]).input_ids))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"oZroWZ1d4-Bo","colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"status":"ok","timestamp":1754013153637,"user_tz":300,"elapsed":196,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"2f55deae-cfb9-4740-ec49-6578b8e69f8d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["sns.histplot(text_lengths)\n","plt.show()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"yhIQNcr7JfTG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013219397,"user_tz":300,"elapsed":65759,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"80bd8d94-ae99-47e0-8f71-cf93ddbd7adb"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 42%|     | 425/1000 [00:34<00:29, 19.81it/s]/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (16) found smaller than n_clusters (18). Possibly due to duplicate points in X.\n","  return fit_method(estimator, *args, **kwargs)\n","100%|| 1000/1000 [01:05<00:00, 15.20it/s]\n"]}],"source":["chunk_size = 1000\n","text_lengths = []\n","\n","for record in tqdm(islice(billsum_train, chunk_size), total = chunk_size):\n","  compressed = compress_bill(record[\"text\"], 18)  # added line to show what input size would look like post-compression\n","  text_lengths.append(len(tokenizer(compressed).input_ids))"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"rozMQ4ouKGcg","colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"status":"ok","timestamp":1754013219599,"user_tz":300,"elapsed":182,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"33284d2c-8ef3-4ad8-df5e-9a19046d959f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["sns.histplot(text_lengths)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vsvuLje2R4gO"},"source":["## Load LED Model"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"BT_FxExaOdkN","executionInfo":{"status":"ok","timestamp":1754013231235,"user_tz":300,"elapsed":2591,"user":{"displayName":"Chris John","userId":"17204747059348754073"}}},"outputs":[],"source":["model = LEDForConditionalGeneration.from_pretrained(model_name).to(DEVICE).eval()\n","\n","config = AutoConfig.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"2Rd6TaL6U73M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013231259,"user_tz":300,"elapsed":21,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"78612c13-d87c-4b78-e203-50c8c74d9fa1"},"outputs":[{"output_type":"stream","name":"stdout","text":["LEDForConditionalGeneration(\n","  (led): LEDModel(\n","    (shared): Embedding(50265, 768, padding_idx=1)\n","    (encoder): LEDEncoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x LEDEncoderLayer(\n","          (self_attn): LEDEncoderAttention(\n","            (longformer_self_attn): LEDEncoderSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (query_global): Linear(in_features=768, out_features=768, bias=True)\n","              (key_global): Linear(in_features=768, out_features=768, bias=True)\n","              (value_global): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (output): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): LEDDecoder(\n","      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n","      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x LEDDecoderLayer(\n","          (self_attn): LEDDecoderAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): LEDDecoderAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","metadata":{"id":"GHaj9Wr0SATH"},"source":["## Test Base LED Without Fine Tuning"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xNX4rM_sRRiw","executionInfo":{"status":"ok","timestamp":1754013235563,"user_tz":300,"elapsed":6,"user":{"displayName":"Chris John","userId":"17204747059348754073"}}},"outputs":[],"source":["def _prepare_batch(batch_texts, max_len_in = 4096):\n","    \"\"\"\n","    Tokenise and attach the global-attention mask required by LED.\n","    Returns a dict ready for model.generate().\n","    \"\"\"\n","    enc = tokenizer(\n","        batch_texts,\n","        padding = \"longest\",\n","        truncation = True,\n","        max_length = max_len_in,\n","        return_tensors = \"pt\"\n","        ).to(DEVICE)\n","\n","    ga_mask = torch.zeros_like(enc[\"input_ids\"])\n","    ga_mask[:, 0] = 1\n","    enc[\"global_attention_mask\"] = ga_mask\n","    return enc\n","\n","def generate_led_summaries(data_texts, batch_size = 2, max_len_in = 4096, max_len_out = 288, min_len_out = 48, num_beam = 4, length_pen = 0.8):\n","    outputs = []\n","    for i in tqdm(range(0, len(data_texts), batch_size)):\n","        batch  = data_texts[i: i + batch_size]\n","        inputs = _prepare_batch(batch, max_len_in = max_len_in)\n","\n","        with torch.no_grad():\n","            ids = model.generate(\n","                **inputs,\n","                num_beams = num_beam,\n","                length_penalty = length_pen,\n","                max_length = max_len_out,\n","                min_length = min_len_out,\n","                no_repeat_ngram_size = 3,\n","                repetition_penalty = 1.05,)\n","\n","        outputs += tokenizer.batch_decode(ids, skip_special_tokens = True, clean_up_tokenization_spaces = False)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"6j2ygEb9ZTUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013645124,"user_tz":300,"elapsed":405527,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"74e97322-3177-4404-ef10-243c07e9e7b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/50 [00:00<?, ?it/s]Input ids are automatically padded from 2539 to 3072 to be a multiple of `config.attention_window`: 1024\n","`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n","`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n","  4%|         | 2/50 [00:16<06:31,  8.17s/it]Input ids are automatically padded from 3583 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 12%|        | 6/50 [00:50<06:20,  8.66s/it]Input ids are automatically padded from 3194 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 20%|        | 10/50 [01:25<05:46,  8.65s/it]Input ids are automatically padded from 2508 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 22%|       | 11/50 [01:31<05:14,  8.07s/it]Input ids are automatically padded from 2593 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 24%|       | 12/50 [01:39<04:55,  7.78s/it]Input ids are automatically padded from 3529 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 26%|       | 13/50 [01:47<04:52,  7.90s/it]Input ids are automatically padded from 3915 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 30%|       | 15/50 [02:04<04:49,  8.26s/it]Input ids are automatically padded from 3346 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 34%|      | 17/50 [02:21<04:36,  8.38s/it]Input ids are automatically padded from 2755 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 36%|      | 18/50 [02:27<04:12,  7.90s/it]Input ids are automatically padded from 3471 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 40%|      | 20/50 [02:44<04:06,  8.23s/it]Input ids are automatically padded from 1900 to 2048 to be a multiple of `config.attention_window`: 1024\n"," 44%|     | 22/50 [02:59<03:41,  7.89s/it]Input ids are automatically padded from 3537 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 48%|     | 24/50 [03:17<03:38,  8.39s/it]Input ids are automatically padded from 2017 to 2048 to be a multiple of `config.attention_window`: 1024\n"," 52%|    | 26/50 [03:32<03:12,  8.04s/it]Input ids are automatically padded from 3554 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 54%|    | 27/50 [03:40<03:05,  8.09s/it]Input ids are automatically padded from 2170 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 60%|    | 30/50 [04:04<02:44,  8.23s/it]Input ids are automatically padded from 3163 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 62%|   | 31/50 [04:12<02:34,  8.11s/it]Input ids are automatically padded from 3485 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 70%|   | 35/50 [04:46<02:08,  8.57s/it]Input ids are automatically padded from 1450 to 2048 to be a multiple of `config.attention_window`: 1024\n"," 74%|  | 37/50 [05:01<01:44,  8.06s/it]Input ids are automatically padded from 2712 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 76%|  | 38/50 [05:08<01:32,  7.67s/it]Input ids are automatically padded from 3325 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 78%|  | 39/50 [05:15<01:25,  7.76s/it]Input ids are automatically padded from 2096 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 82%| | 41/50 [05:30<01:09,  7.72s/it]Input ids are automatically padded from 2913 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 84%| | 42/50 [05:38<01:00,  7.61s/it]Input ids are automatically padded from 3911 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 86%| | 43/50 [05:46<00:55,  7.96s/it]Input ids are automatically padded from 2900 to 3072 to be a multiple of `config.attention_window`: 1024\n"," 94%|| 47/50 [06:20<00:25,  8.38s/it]Input ids are automatically padded from 3440 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 96%|| 48/50 [06:28<00:16,  8.29s/it]Input ids are automatically padded from 4002 to 4096 to be a multiple of `config.attention_window`: 1024\n"," 98%|| 49/50 [06:37<00:08,  8.56s/it]Input ids are automatically padded from 3545 to 4096 to be a multiple of `config.attention_window`: 1024\n","100%|| 50/50 [06:45<00:00,  8.11s/it]\n"]}],"source":["sample = compressed_test_list[:100]\n","test_summaries_base = generate_led_summaries(sample)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"IoOeMncLo3--","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013646684,"user_tz":300,"elapsed":1545,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"a6f78c5a-4807-4a69-a9f7-a61772c81c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'rouge1': np.float64(0.3413294778106509), 'rouge2': np.float64(0.12988156676713908), 'rougeL': np.float64(0.20556118480902408), 'rougeLsum': np.float64(0.23849941823876997)}\n"]}],"source":["n = len(sample)\n","references = [billsum_test[i][\"summary\"] for i in range(n)]\n","\n","results = rouge.compute(\n","    predictions = test_summaries_base,\n","    references = references)\n","\n","print(results)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"XXJxWK-6qWTX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013656004,"user_tz":300,"elapsed":9319,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"65ca5ab9-21e3-4026-c9ac-85e7c39c1e4f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n","  return forward_call(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["{'precision': [0.7649412155151367, 0.7817134857177734, 0.7782076597213745, 0.7174849510192871, 0.7778568863868713, 0.7779546976089478, 0.7435086965560913, 0.7422400712966919, 0.7612226009368896, 0.744891881942749, 0.7132190465927124, 0.7586778402328491, 0.7524328231811523, 0.7797965407371521, 0.7816134095191956, 0.7883825302124023, 0.7571128606796265, 0.716232180595398, 0.8116205334663391, 0.7329022288322449, 0.7829716205596924, 0.8242081999778748, 0.732672929763794, 0.8079277276992798, 0.7470591068267822, 0.8182104825973511, 0.8155552744865417, 0.7994697093963623, 0.737178385257721, 0.766051173210144, 0.7381647229194641, 0.8046316504478455, 0.7828664779663086, 0.7516739368438721, 0.7567933797836304, 0.7768277525901794, 0.7580491304397583, 0.7738962173461914, 0.8288862705230713, 0.7137699127197266, 0.8071317672729492, 0.8459096550941467, 0.728501558303833, 0.7349412441253662, 0.7785330414772034, 0.7519441843032837, 0.7480026483535767, 0.8057172298431396, 0.8000333309173584, 0.7837666273117065, 0.7388736009597778, 0.7770642042160034, 0.7214193940162659, 0.7339944243431091, 0.7963036894798279, 0.8010125160217285, 0.7486382722854614, 0.7111169099807739, 0.7466921806335449, 0.7379885911941528, 0.7844464182853699, 0.7513556480407715, 0.7351727485656738, 0.8112422227859497, 0.773366391658783, 0.7335585355758667, 0.767776370048523, 0.759913980960846, 0.7458151578903198, 0.781812310218811, 0.7675663232803345, 0.8240326642990112, 0.7365404367446899, 0.7325872778892517, 0.7588027715682983, 0.8046953678131104, 0.801509439945221, 0.7996494770050049, 0.7521778345108032, 0.7735759019851685, 0.7595220804214478, 0.8161801099777222, 0.7807217240333557, 0.7980992794036865, 0.7999330759048462, 0.7481755614280701, 0.8165138959884644, 0.7519028782844543, 0.7677534818649292, 0.7443257570266724, 0.7259144186973572, 0.819259524345398, 0.7543277740478516, 0.7468191981315613, 0.7160191535949707, 0.7437468767166138, 0.762173056602478, 0.7741003632545471, 0.8083885908126831, 0.8021173477172852], 'recall': [0.8001095056533813, 0.8554067611694336, 0.8446916937828064, 0.8103765249252319, 0.8129133582115173, 0.8628072738647461, 0.8353173136711121, 0.7910070419311523, 0.8201484680175781, 0.8020756244659424, 0.7879111766815186, 0.8174951672554016, 0.8214653730392456, 0.8290786147117615, 0.8091090321540833, 0.8345184326171875, 0.8406018018722534, 0.7947050333023071, 0.8360131978988647, 0.8462945818901062, 0.7982230186462402, 0.8034951686859131, 0.8187819719314575, 0.8312000036239624, 0.8474363684654236, 0.8572509288787842, 0.832981526851654, 0.8331509828567505, 0.8244236707687378, 0.861088752746582, 0.8128713965415955, 0.8609422445297241, 0.8383006453514099, 0.8323002457618713, 0.8044679760932922, 0.847621738910675, 0.7969101667404175, 0.8252182006835938, 0.8199886083602905, 0.8069658279418945, 0.8257805705070496, 0.8595314621925354, 0.8148453235626221, 0.7717731595039368, 0.7812483906745911, 0.8686039447784424, 0.8407396674156189, 0.8679162263870239, 0.8275126814842224, 0.8304174542427063, 0.8635481595993042, 0.8456873893737793, 0.799446702003479, 0.772178590297699, 0.8206167221069336, 0.8429659008979797, 0.8230931758880615, 0.8396490812301636, 0.7828524112701416, 0.8316795825958252, 0.8174872398376465, 0.8716081380844116, 0.8118504285812378, 0.8522599339485168, 0.8481394052505493, 0.853734016418457, 0.8088210225105286, 0.8287911415100098, 0.8449498414993286, 0.8201597929000854, 0.8691378235816956, 0.8198730945587158, 0.8050304055213928, 0.8099527359008789, 0.8159655332565308, 0.864850640296936, 0.8063169717788696, 0.8374190330505371, 0.8402514457702637, 0.8320186734199524, 0.810242772102356, 0.8579553365707397, 0.7946246862411499, 0.8449957966804504, 0.8251705765724182, 0.8304639458656311, 0.8258680105209351, 0.7746632695198059, 0.7916690707206726, 0.7576996088027954, 0.834591269493103, 0.8233385682106018, 0.7909185886383057, 0.7940454483032227, 0.8464888334274292, 0.7969510555267334, 0.8066291809082031, 0.819612979888916, 0.862138032913208, 0.8048103451728821], 'f1': [0.782130241394043, 0.8169015049934387, 0.8100879192352295, 0.7611068487167358, 0.794998824596405, 0.81818687915802, 0.786743700504303, 0.7658480405807495, 0.789587676525116, 0.7724268436431885, 0.7487068772315979, 0.7869890332221985, 0.785435140132904, 0.803682804107666, 0.7951235771179199, 0.8107947111129761, 0.796675980091095, 0.7534307837486267, 0.8236362934112549, 0.7855274081230164, 0.7905237674713135, 0.8137198686599731, 0.7733378410339355, 0.8193987011909485, 0.7940882444381714, 0.8372758626937866, 0.8241762518882751, 0.8159629106521606, 0.778363823890686, 0.810794472694397, 0.7737188935279846, 0.8318350911140442, 0.8096357583999634, 0.7899351119995117, 0.7799027562141418, 0.8106821179389954, 0.7769940495491028, 0.798733651638031, 0.8244134187698364, 0.7575122117996216, 0.8163496851921082, 0.8526661396026611, 0.7692581415176392, 0.7529070377349854, 0.7798883318901062, 0.8060750365257263, 0.7916646003723145, 0.8356609344482422, 0.8135409951210022, 0.8064179420471191, 0.7963607907295227, 0.809924840927124, 0.7584314346313477, 0.7526025176048279, 0.808277428150177, 0.821453869342804, 0.7841022610664368, 0.7700563669204712, 0.7643448710441589, 0.7820379734039307, 0.8006260395050049, 0.8070268630981445, 0.7716113924980164, 0.8312453627586365, 0.8090289235115051, 0.7890969514846802, 0.7877644300460815, 0.7928595542907715, 0.7922935485839844, 0.8005270957946777, 0.8152003884315491, 0.8219475746154785, 0.7692639827728271, 0.769329845905304, 0.7863466739654541, 0.8336893320083618, 0.8039060235023499, 0.8180985450744629, 0.7937790751457214, 0.8017336130142212, 0.7840629816055298, 0.8365465402603149, 0.7876118421554565, 0.8208783268928528, 0.8123558163642883, 0.7871750593185425, 0.8211643099784851, 0.7631134390830994, 0.7795279026031494, 0.7509531378746033, 0.7764686346054077, 0.8212939500808716, 0.7721899747848511, 0.7697085738182068, 0.7758069038391113, 0.7694303393363953, 0.7837712168693542, 0.7962068319320679, 0.834398627281189, 0.8034616112709045], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.54.1)'}\n"]}],"source":["results = bertscore.compute(\n","    predictions = test_summaries_base,\n","    references = references,\n","    lang = \"en\")\n","\n","print(results)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"hLyYlMTCtJN4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754013735150,"user_tz":300,"elapsed":42,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"fd06f301-eec4-443b-d64c-45fd0c5cd5b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average precision: 0.7682805269956589\n","Average recall: 0.8246724671125412\n","Average f1: 0.7951254832744599\n"]}],"source":["print(f'Average precision: {np.mean(results[\"precision\"])}')\n","print(f'Average recall: {np.mean(results[\"recall\"])}')\n","print(f'Average f1: {np.mean(results[\"f1\"])}')"]},{"cell_type":"markdown","source":["## Create Trainer Cell\n","\n","###### -- Builds mini batches, pads them to the longest item in the batch, and shifts target tokens so model can compute loss that way.\n","\n","###### -- Hyper-parameter tuning done to specify batch size, epochs, learning rate, and logging steps."],"metadata":{"id":"u1v-1g35COlt"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224,"referenced_widgets":["995940034ee84670b36c0c7a8d07b153","b9b4ff5c8935409f96a991ef9d3db3f8","0714161885c9455591e6aeba15356229","5046051340a24d4b85952eab562cd781","0d19a29c275d416ea8ff940cdd51213e","2e8c68292636497d9f3b54bcd5fcdeff","e413c2cb7bc946e191afdc1d48c79c81","57ebe259ac104b919947a79f0f694701","64c1ef14b1c64eab8c9b6a8b199cacf4","5eef20a9c0a2482782af4cc2d5663b59","34ee4aa651604d238b215d11e32c742b","d2dfbdffe8214beba2dd422cc3867eac","a1dd155c153247fa8d6bf189dc8c3991","3b4ac9bff41b44e8a8e7565040b6a1d0","09285da52a0b457486a193d950f9f3ba","dddcf5711fe149439983d39f8ebc8d4b","f27ebb1d894e4b46a0267118a2f8585a","6f10b67b1a38406ba34758d5498dc507","4d6b969c2f7b44f7915bf2dcc77d4b89","e03d653390f34c8bb9803d55464b80b6","acbcb1daeaa349ce8d05033c2510f2f9","f8f99ab00d5442aca15f4c704ad42147"]},"id":"6a8a4978","executionInfo":{"status":"ok","timestamp":1754014366560,"user_tz":300,"elapsed":105453,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"422f06c6-7412-4905-df57-1d54319d97de"},"source":["def preprocess_function(examples):\n","    # Tokenize the text\n","    model_inputs = tokenizer(examples[\"text\"], max_length = 4096, truncation = True)\n","\n","    # Tokenize the summaries (as labels)\n","    labels = tokenizer(examples[\"summary\"], max_length = 288, truncation = True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","# Take a slice of the training data and convert to a standard Dataset\n","train_slice_size = 1000\n","train_data_list = list(islice(billsum_train, train_slice_size))\n","train_dataset_slice = datasets.Dataset.from_list(train_data_list)\n","\n","# Apply the preprocessing function to the datasets\n","tokenized_train_data = train_dataset_slice.map(preprocess_function, batched = True)\n","tokenized_val_data = billsum_test.map(preprocess_function, batched = True)\n","\n","# Remove original text and summary columns\n","tokenized_train_data = tokenized_train_data.remove_columns([\"text\", \"summary\", \"title\"])\n","tokenized_val_data = tokenized_val_data.remove_columns([\"text\", \"summary\", \"title\"])\n","\n","print(\"Tokenized training data:\", tokenized_train_data)\n","print(\"Tokenized validation data:\", tokenized_val_data)"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995940034ee84670b36c0c7a8d07b153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/3269 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2dfbdffe8214beba2dd422cc3867eac"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokenized training data: Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 1000\n","})\n","Tokenized validation data: Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 3269\n","})\n"]}]},{"cell_type":"code","source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\n","\n","args = Seq2SeqTrainingArguments(\n","    output_dir = \"led_billsum_ft\",\n","    per_device_train_batch_size = 1,\n","    per_device_eval_batch_size = 1,\n","    gradient_accumulation_steps = 4,\n","    num_train_epochs = 1,\n","    learning_rate = 5e-5,\n","    eval_strategy = \"epoch\",\n","    predict_with_generate = True,\n","    logging_steps = 200,\n","    max_steps = 5000,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model = model,\n","    args = args,\n","    train_dataset = tokenized_train_data,\n","    eval_dataset = tokenized_val_data,\n","    data_collator = data_collator\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qsyWEXBO-PGZ","executionInfo":{"status":"error","timestamp":1754016313591,"user_tz":300,"elapsed":1695037,"user":{"displayName":"Chris John","userId":"17204747059348754073"}},"outputId":"ff544ccb-650a-471b-bfb1-6ad5444cf8ab"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Input ids are automatically padded from 2491 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2325 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3539 to 4096 to be a multiple of `config.attention_window`: 1024\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='251' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [250/250 20:18, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='1717' max='3269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1717/3269 07:32 < 06:49, 3.79 it/s]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Input ids are automatically padded from 2019 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3046 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2022 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2984 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2317 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2697 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3184 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2138 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2548 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1795 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2446 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1984 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1950 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3966 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1735 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1968 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1707 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3728 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3988 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3209 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2473 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2468 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3752 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3438 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2309 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4057 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3067 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3244 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2424 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4045 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3734 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3037 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3357 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1936 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2629 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2867 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2074 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2799 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1539 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3840 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1560 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2691 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2694 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3763 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2228 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3932 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2631 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1700 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3301 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2756 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3347 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2265 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2841 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1298 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4061 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2224 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2321 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3052 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4006 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2905 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2834 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3975 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2357 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2957 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3442 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3035 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2520 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3507 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3853 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4095 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1935 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2422 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3212 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3531 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2676 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4070 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1729 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2063 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3257 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3352 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3546 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2202 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4087 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2106 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2939 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2572 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1944 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2191 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1793 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3019 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3026 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1660 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1917 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2329 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1824 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1889 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4037 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2823 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2958 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1575 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1647 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3617 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1726 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3363 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2882 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2715 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2693 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3152 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3348 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2348 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2380 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3031 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2636 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2874 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2789 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2212 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3974 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1853 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1496 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2088 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3707 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1986 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2286 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1933 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3684 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3935 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2270 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1945 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3941 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3875 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2041 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2187 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1723 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1937 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2181 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2869 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2795 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1857 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2085 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2711 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2227 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3139 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2554 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3446 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2099 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2503 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3273 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2603 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3140 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2634 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3652 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2464 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4011 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3958 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2881 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3860 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2253 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1738 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2993 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3653 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1988 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1287 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3705 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2776 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1469 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1801 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3754 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2454 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3811 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1627 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1806 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1871 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2429 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2922 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3083 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2521 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3702 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2536 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3884 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3863 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4059 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1952 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2951 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3821 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2136 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3992 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3122 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3473 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3365 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3455 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3417 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1918 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4068 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3657 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3999 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3108 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2793 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2722 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2769 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3332 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3873 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2391 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2042 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2983 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1931 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2649 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3503 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2990 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1995 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3448 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1624 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1800 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1646 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2729 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3078 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2618 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2082 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1766 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2884 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1642 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1580 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1842 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3249 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2822 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3815 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2412 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2597 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2570 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3059 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3564 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1861 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2525 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3421 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1831 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1926 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2522 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3432 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3761 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1748 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3711 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3809 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4042 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2476 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3727 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1888 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3733 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3315 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2736 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2168 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2552 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3241 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2115 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2229 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3557 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2266 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2169 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2210 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2298 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2587 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3135 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2157 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2501 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1628 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2932 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1606 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3428 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2332 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2263 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2537 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3589 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2847 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1692 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3399 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3796 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2179 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1980 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2784 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3372 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2687 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4012 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2596 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3387 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1693 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2301 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2829 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2490 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1887 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2865 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4016 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3383 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3597 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3321 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3661 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2488 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3973 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3624 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1719 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1747 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3918 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2931 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3674 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1668 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2294 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2770 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2248 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3172 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1711 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3817 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2758 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1791 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3675 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2494 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3150 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2184 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2268 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2751 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2000 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2730 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1730 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3034 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1664 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2486 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1852 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2706 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2132 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3605 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2549 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3196 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1886 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2462 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2326 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3155 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2427 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1813 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3289 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3141 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2519 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1928 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1969 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1788 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2902 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3154 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2445 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3659 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1622 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1840 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2252 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1521 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3006 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3751 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1953 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3117 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3693 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1925 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4019 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4069 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2558 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3367 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1947 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2408 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2430 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1616 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2195 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1882 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3361 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3048 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2806 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2058 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3167 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3577 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3804 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2995 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3330 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3189 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2206 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1291 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2956 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1457 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2069 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1749 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3303 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3369 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1595 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3478 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2547 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1363 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1525 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2354 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2297 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3868 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1880 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1949 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3777 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2032 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3214 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3519 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3371 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3391 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3593 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2287 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2043 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2134 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3696 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2836 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3285 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3846 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3928 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2192 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3268 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1442 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4033 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2423 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3130 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3534 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2044 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1586 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2002 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4076 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2801 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3629 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2839 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2197 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1619 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3698 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3464 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4052 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2375 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3402 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2561 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2860 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3115 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3113 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2172 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2441 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4094 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2478 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2261 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2633 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3646 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3561 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3971 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2915 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1905 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1724 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4014 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2247 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1753 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3304 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2156 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1395 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2322 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4005 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2211 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2183 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2665 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2303 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1743 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1910 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2003 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2386 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4082 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2083 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2241 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3622 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2218 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2942 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3676 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3445 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2705 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3175 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2262 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3225 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2038 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3377 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3143 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2656 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1557 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2432 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2438 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2308 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3341 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3168 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3410 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2897 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3215 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3690 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2123 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1607 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3919 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3472 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2146 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1939 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3903 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2455 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4008 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3781 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2911 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3132 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3208 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2657 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1276 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2318 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3356 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2372 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3049 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2336 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3986 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2645 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1431 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2199 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2246 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3238 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2026 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2023 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4001 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3611 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2379 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1815 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1759 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3843 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1836 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2276 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2016 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3057 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2779 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2031 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2094 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2416 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3740 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2930 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3138 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2996 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2351 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2420 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3914 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2718 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2928 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1565 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2051 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3543 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2413 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1896 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2710 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3588 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3953 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2873 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3137 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1650 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3013 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3099 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2154 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2870 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2833 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1965 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2652 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3797 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2624 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3011 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3162 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2281 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2642 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3950 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2846 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3551 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3993 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3054 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2353 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2314 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2140 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2588 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1957 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3633 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2204 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3314 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2087 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3227 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3612 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2892 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2613 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1812 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3575 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1640 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2117 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3937 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1737 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3465 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1822 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1769 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3584 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2254 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3897 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1702 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1924 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1386 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3578 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3521 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1767 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3199 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1604 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3523 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1948 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2726 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1989 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1630 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1584 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2116 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1482 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2929 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3939 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1993 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2451 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2562 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3493 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2417 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3514 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2119 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2275 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3323 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2118 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2092 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2279 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3370 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3933 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2457 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3076 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3294 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1600 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2509 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3718 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1943 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2251 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3045 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2425 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2977 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1960 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3338 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2363 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2165 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2972 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3956 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3028 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2632 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3062 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3039 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2628 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3558 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1976 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1955 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4034 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2452 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1574 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1883 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2222 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3783 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2474 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2046 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2563 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2264 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1725 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3453 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1639 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3810 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3579 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3102 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3284 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2105 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2203 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3800 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2527 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2185 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2061 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3178 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3560 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3271 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3772 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2844 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3381 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1870 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2233 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2696 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3419 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1821 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2392 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1881 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2817 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2307 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2878 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2057 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2919 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2419 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2237 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2171 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2518 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3708 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1695 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3290 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3050 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3964 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3520 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1833 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2055 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2699 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3192 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2907 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3481 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3954 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3235 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2994 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3606 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3501 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1653 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2208 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3785 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2638 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1785 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1966 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2068 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1665 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2859 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1911 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2249 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3994 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2006 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2018 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2304 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2871 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3483 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1378 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1398 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2832 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1671 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1941 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3625 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2448 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1843 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2384 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3526 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2277 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1927 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2546 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2219 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3193 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3090 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1680 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2903 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3179 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2923 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2855 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1780 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3105 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3917 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1777 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2089 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3089 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3566 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4041 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2485 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2364 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2883 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1923 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2771 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3408 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1512 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3533 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2370 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1999 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2122 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2979 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3627 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3851 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1938 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3667 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3559 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2567 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3891 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2599 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2155 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1417 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2009 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2553 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2692 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2876 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1579 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2616 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2540 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2606 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2760 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3555 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1962 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2111 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2449 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1974 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2143 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2647 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2748 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1914 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2680 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3726 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3908 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2175 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1786 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2418 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2849 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1876 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3620 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3852 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1571 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2610 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1940 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3229 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3867 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2064 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1841 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1728 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2640 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3127 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1552 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1528 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3016 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2067 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2868 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2857 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2788 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1487 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2107 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2167 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2361 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2108 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3981 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2766 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2141 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1641 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3439 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2200 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2121 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2214 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1550 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2875 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3169 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2110 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1961 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3030 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3395 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2581 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1762 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3272 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3205 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2284 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1734 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2496 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3710 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3077 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2289 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3068 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1768 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1591 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3458 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1596 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3881 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3270 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3406 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2013 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2296 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2792 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2302 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3862 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2497 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2532 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2283 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2818 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2299 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2514 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3246 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1875 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1666 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3484 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3334 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2980 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4089 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2564 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2193 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2426 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2428 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1611 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2659 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2732 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3288 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3239 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1981 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3587 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1625 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3112 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1626 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2081 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1742 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1805 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2506 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3223 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2145 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1699 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2998 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3630 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3663 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3027 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2273 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1661 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3404 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1350 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2347 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3326 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3286 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2541 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3926 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3823 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2398 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2065 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2530 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1783 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2098 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2749 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3204 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2153 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2604 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3409 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3945 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2027 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3217 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1662 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2830 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3230 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3604 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3358 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3552 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2104 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2740 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2679 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2052 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1807 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1617 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2050 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4055 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3729 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2075 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2177 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2290 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1563 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2037 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3631 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3770 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3310 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3400 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2512 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3599 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3080 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2901 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3991 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4073 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1648 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3984 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3008 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1732 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2250 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2331 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3495 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2856 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1696 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1439 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2402 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3201 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3106 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1858 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2399 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4023 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2459 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1827 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1713 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1555 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2395 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4067 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3148 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2586 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2787 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2666 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2320 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2033 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1942 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2059 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3123 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2244 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1909 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2671 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1884 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3938 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1897 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3032 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1860 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3920 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2885 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2152 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3171 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3297 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1718 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3474 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1921 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3585 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3431 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1757 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2149 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3343 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3946 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2090 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3313 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2103 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3131 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1499 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2188 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2947 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2477 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2282 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2343 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1891 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3508 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1998 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1511 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2566 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2772 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3145 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3224 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2324 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3689 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1495 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1977 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2341 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1922 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3418 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4000 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1773 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3842 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3488 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2220 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3921 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2306 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3692 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3444 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2040 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3110 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2498 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3231 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4091 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1837 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3607 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1679 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1645 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2999 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2612 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2144 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3392 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2798 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2182 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2225 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2230 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1803 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2489 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2675 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3979 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2256 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1264 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1987 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2543 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1428 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2635 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2960 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2590 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2854 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1761 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1635 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3619 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1716 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2047 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1825 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1963 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2602 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3101 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2644 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4030 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2589 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3877 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2637 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3058 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2690 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2985 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1744 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2507 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2848 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2707 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3133 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3360 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1794 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3463 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3524 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2909 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3496 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3532 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3951 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2992 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2886 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1678 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3210 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3820 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3832 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3714 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2889 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3466 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3716 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3839 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3573 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2114 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2164 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2970 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3912 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3198 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1691 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1498 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1990 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1901 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2014 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1893 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1771 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3156 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3411 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2544 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2327 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2670 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1975 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2600 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2982 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1814 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3799 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3685 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1461 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1633 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1750 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2095 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2643 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3374 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1855 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2971 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1180 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3311 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2786 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1979 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2974 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2358 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3596 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1895 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4003 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4010 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2130 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3826 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3904 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2850 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1863 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3066 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2447 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3494 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3254 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2630 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3686 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2910 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3309 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3475 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3200 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2661 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2551 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2272 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2820 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1829 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2906 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3043 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3337 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2879 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2073 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2838 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1667 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3898 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2997 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3788 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2334 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1983 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2987 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2112 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2813 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3691 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1878 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1894 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1810 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4075 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1959 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2345 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1967 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1834 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3469 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3934 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1845 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2137 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2966 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1567 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2080 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3704 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3808 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2592 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2921 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1946 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2622 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3679 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2991 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3515 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1475 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2030 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3719 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2502 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3568 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2684 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3467 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1868 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1601 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2293 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1862 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2315 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3874 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3378 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2258 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 4053 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3479 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2861 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3100 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1672 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1573 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3118 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2342 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2344 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1830 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3379 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3610 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2131 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2466 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2583 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2359 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3669 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2078 to 3072 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 1892 to 2048 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 3278 to 4096 to be a multiple of `config.attention_window`: 1024\n","Input ids are automatically padded from 2914 to 3072 to be a multiple of `config.attention_window`: 1024\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3708036394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2238\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2694\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2695\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3131\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3133\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3134\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     def predict(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4250\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4433\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4434\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4435\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4436\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"npu:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# .to() doesn't accept non_blocking as kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             self.data = {\n\u001b[0m\u001b[1;32m    810\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             self.data = {\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             }\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model.save_pretrained(\"led_billsum_ft\")\n","tokenizer.save_pretrained(\"led_billsum_ft\")"],"metadata":{"id":"X6AmPT7_Dnmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","sample = compressed_test_list[:100]\n","ft_summaries = generate_led_summaries(sample)"],"metadata":{"id":"vDTZFFSUDos8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = len(sample)\n","references = [billsum_test[i][\"summary\"] for i in range(n)]\n","\n","results_fine_tuned = rouge.compute(\n","    predictions = ft_summaries,\n","    references = references)\n","\n","print(results_fine_tuned)"],"metadata":{"id":"tn6ZRJNSELgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_fine_tuned = bertscore.compute(\n","    predictions = ft_summaries,\n","    references = references,\n","    lang = \"en\")\n","\n","print(results_fine_tuned)"],"metadata":{"id":"wfbcxQbJEbIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Average precision: {np.mean(results[\"precision\"])}')\n","print(f'Average recall: {np.mean(results[\"recall\"])}')\n","print(f'Average f1: {np.mean(results[\"f1\"])}')"],"metadata":{"id":"Vj7kgNpYEc75"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1SEEEltOp1zTOPrhczGUqITpK-fpTsHua","timestamp":1752876170028}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"995940034ee84670b36c0c7a8d07b153":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9b4ff5c8935409f96a991ef9d3db3f8","IPY_MODEL_0714161885c9455591e6aeba15356229","IPY_MODEL_5046051340a24d4b85952eab562cd781"],"layout":"IPY_MODEL_0d19a29c275d416ea8ff940cdd51213e"}},"b9b4ff5c8935409f96a991ef9d3db3f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e8c68292636497d9f3b54bcd5fcdeff","placeholder":"","style":"IPY_MODEL_e413c2cb7bc946e191afdc1d48c79c81","value":"Map:100%"}},"0714161885c9455591e6aeba15356229":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57ebe259ac104b919947a79f0f694701","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64c1ef14b1c64eab8c9b6a8b199cacf4","value":1000}},"5046051340a24d4b85952eab562cd781":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5eef20a9c0a2482782af4cc2d5663b59","placeholder":"","style":"IPY_MODEL_34ee4aa651604d238b215d11e32c742b","value":"1000/1000[00:21&lt;00:00,47.31examples/s]"}},"0d19a29c275d416ea8ff940cdd51213e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e8c68292636497d9f3b54bcd5fcdeff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e413c2cb7bc946e191afdc1d48c79c81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57ebe259ac104b919947a79f0f694701":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64c1ef14b1c64eab8c9b6a8b199cacf4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5eef20a9c0a2482782af4cc2d5663b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34ee4aa651604d238b215d11e32c742b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2dfbdffe8214beba2dd422cc3867eac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1dd155c153247fa8d6bf189dc8c3991","IPY_MODEL_3b4ac9bff41b44e8a8e7565040b6a1d0","IPY_MODEL_09285da52a0b457486a193d950f9f3ba"],"layout":"IPY_MODEL_dddcf5711fe149439983d39f8ebc8d4b"}},"a1dd155c153247fa8d6bf189dc8c3991":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f27ebb1d894e4b46a0267118a2f8585a","placeholder":"","style":"IPY_MODEL_6f10b67b1a38406ba34758d5498dc507","value":"Map:100%"}},"3b4ac9bff41b44e8a8e7565040b6a1d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d6b969c2f7b44f7915bf2dcc77d4b89","max":3269,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e03d653390f34c8bb9803d55464b80b6","value":3269}},"09285da52a0b457486a193d950f9f3ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acbcb1daeaa349ce8d05033c2510f2f9","placeholder":"","style":"IPY_MODEL_f8f99ab00d5442aca15f4c704ad42147","value":"3269/3269[01:09&lt;00:00,47.26examples/s]"}},"dddcf5711fe149439983d39f8ebc8d4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f27ebb1d894e4b46a0267118a2f8585a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f10b67b1a38406ba34758d5498dc507":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d6b969c2f7b44f7915bf2dcc77d4b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e03d653390f34c8bb9803d55464b80b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"acbcb1daeaa349ce8d05033c2510f2f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8f99ab00d5442aca15f4c704ad42147":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}