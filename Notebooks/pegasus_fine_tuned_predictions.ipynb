{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdde679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.amp import autocast\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "import json\n",
    "from math import ceil\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0492ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "MODEL_NAME = \"./bigbird_pegasus_fine_tune/checkpoint-80\"\n",
    "BATCH_SIZE = 32\n",
    "MAX_INPUT_LEN = 4096\n",
    "MAX_OUTPUT_LEN = 600\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_FIELD = \"text\"  # adjust if your input field is different\n",
    "ID_FIELD = \"title\"\n",
    "OUTPUT_FILE = \"billsum_test_bigbird_tuned_pred.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa39599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): BigBirdPegasusForConditionalGeneration(\n",
       "    (model): BigBirdPegasusModel(\n",
       "      (shared): BigBirdPegasusScaledWordEmbedding(96103, 1024, padding_idx=0)\n",
       "      (encoder): BigBirdPegasusEncoder(\n",
       "        (embed_tokens): BigBirdPegasusScaledWordEmbedding(96103, 1024, padding_idx=0)\n",
       "        (embed_positions): BigBirdPegasusLearnedPositionalEmbedding(4096, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x BigBirdPegasusEncoderLayer(\n",
       "            (self_attn): BigBirdPegasusEncoderAttention(\n",
       "              (self): BigBirdPegasusBlockSparseAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (output): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BigBirdPegasusDecoder(\n",
       "        (embed_tokens): BigBirdPegasusScaledWordEmbedding(96103, 1024, padding_idx=0)\n",
       "        (embed_positions): BigBirdPegasusLearnedPositionalEmbedding(4096, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x BigBirdPegasusDecoderLayer(\n",
       "            (self_attn): BigBirdPegasusDecoderAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BigBirdPegasusDecoderAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load Model and Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bbe779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 records already summarized\n",
      "Dataset length before filter: 3269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8542cbed9a7b4614b536a2e190163979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length after filter: 3045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'bill_id', 'title', 'text_len', 'sum_len'],\n",
       "    num_rows: 3045\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(\"json\", data_files=\"billsum_data/us_test_data_final_OFFICIAL.jsonl\")[\"train\"]\n",
    "\n",
    "# Track already completed summaries\n",
    "completed_ids = set()\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                completed_ids.add(record[ID_FIELD])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Filter out completed records\n",
    "print(len(completed_ids), \"records already summarized\")\n",
    "print(\"Dataset length before filter:\", len(dataset))\n",
    "dataset = dataset.filter(lambda x: str(x[ID_FIELD]) not in completed_ids)\n",
    "print(\"Dataset length after filter:\", len(dataset))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a82c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3de1f53b554aaa96a944d746e3278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'bill_id', 'title', 'text_len', 'sum_len', 'input_ids', 'input_len', 'attention_mask'],\n",
       "    num_rows: 3045\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize all texts\n",
    "def preprocess(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        padding=False\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokens[\"input_ids\"],\n",
    "        \"input_len\": len(tokens[\"input_ids\"]),\n",
    "        \"attention_mask\": tokens[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"text\"])\n",
    "dataset = dataset.sort(\"input_len\", reverse=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d970db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [3:16:50<00:00, 123.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Run Inference ---\n",
    "num_batches = ceil(len(dataset) / BATCH_SIZE)\n",
    "summaries = []\n",
    "all_titles = []\n",
    "with open(OUTPUT_FILE, \"a\") as outfile:\n",
    "    for i in tqdm(range(num_batches), desc=\"Generating summaries\"):\n",
    "        # Extract batch from dataset\n",
    "        start = i * BATCH_SIZE\n",
    "        end = min((i + 1) * BATCH_SIZE, len(dataset))\n",
    "        batch = dataset.select(range(start, end))\n",
    "        \n",
    "        # Pad to max length in batch\n",
    "        padded = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"]\n",
    "            },\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad(), autocast(\"cuda\"):\n",
    "            outputs = model.generate(\n",
    "                input_ids=padded[\"input_ids\"],\n",
    "                attention_mask=padded[\"attention_mask\"],\n",
    "                max_length=MAX_OUTPUT_LEN,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "\n",
    "        for example, summary in zip(batch, decoded):\n",
    "            json.dump({ID_FIELD: example[ID_FIELD], \"pred_summary\": summary}, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
