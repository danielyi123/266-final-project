% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{Eidelman_2019,
   title={BillSum: A Corpus for Automatic Summarization of US Legislation},
   url={http://dx.doi.org/10.18653/v1/D19-5406},
   DOI={10.18653/v1/d19-5406},
   booktitle={Proceedings of the 2nd Workshop on New Frontiers in Summarization},
   publisher={Association for Computational Linguistics},
   author={Anastassia Kornilova and Vladimir Eidelman},
   year={2019},
   pages={48–56} }

@inproceedings{agarwal-etal-2022-extractive,
    title = "Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance",
    author = "Agarwal, Abhishek  and
      Xu, Shanshan  and
      Grabmair, Matthias",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.134/",
    doi = "10.18653/v1/2022.findings-emnlp.134",
    pages = "1857--1872",
    abstract = "Summarizing legal decisions requires the expertise of law practitioners, which is both time- and cost-intensive. This paper presents techniques for extractive summarization of legal decisions in a low-resource setting using limited expert annotated data. We test a set of models that locate relevant content using a sequential model and tackle redundancy by leveraging maximal marginal relevance to compose summaries. We also demonstrate an implicit approach to help train our proposed models generate more informative summaries. Our multi-task learning model variant leverages rhetorical role identification as an auxiliary task to further improve the summarizer. We perform extensive experiments on datasets containing legal decisions from the US Board of Veterans' Appeals and conduct quantitative and expert-ranked evaluations of our models. Our results show that the proposed approaches can achieve ROUGE scores vis-{\`a}-vis expert extracted summaries that match those achieved by inter-annotator comparison."
}

@INPROCEEDINGS{9397119,
  author={Jain, Deepali and Borah, Malaya Dutta and Biswas, Anupam},
  booktitle={2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)}, 
  title={Automatic Summarization of Legal Bills: A Comparative Analysis of Classical Extractive Approaches}, 
  year={2021},
  volume={},
  number={},
  pages={394-400},
  keywords={Measurement;Law;Benchmark testing;Intelligent systems;legal document summarization;legal extractive summarization;comparative analysis;legal domain},
  doi={10.1109/ICCCIS51004.2021.9397119}}

@ARTICLE{Jain2024-ht,
  title     = "A sentence is known by the company it keeps: Improving Legal
               Document Summarization Using Deep Clustering",
  author    = "Jain, Deepali and Borah, Malaya Dutta and Biswas, Anupam",
  journal   = "Artif. Intell. Law",
  publisher = "Springer Science and Business Media LLC",
  volume    =  32,
  number    =  1,
  pages     = "165--200",
  month     =  mar,
  year      =  2024,
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en",
  doi={10.1007/s10506-023-09345-y}
}


@InProceedings{pmlr-v119-zhang20ae,
  title = 	 {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11328--11339},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhang20ae.html},
  abstract = 	 {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.}
}

@misc{zaheer2021bigbirdtransformerslonger,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.14062}, 
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}